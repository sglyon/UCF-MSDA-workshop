{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression tasks with sklearn\n",
    "\n",
    "In this notebook we will explore various algorithms for doing regression tasks with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "url = \"https://storage.googleapis.com/qeds/data/kc_house_data.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "X = df.drop([\"price\", \"date\", \"id\"], axis=1).copy()\n",
    "# convert everything to be a float for later on\n",
    "X = X.astype(float)\n",
    "\n",
    "# notice the log here!\n",
    "y = np.log(df[\"price\"])\n",
    "df[\"log_price\"] = y\n",
    "\n",
    "def var_scatter(df, ax=None, var=\"sqft_living\"):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(figsize=(8, 6))\n",
    "    df.plot.scatter(x=var , y=\"log_price\", alpha=0.35, s=1.5, ax=ax)\n",
    "\n",
    "    return ax\n",
    "\n",
    "var_scatter(df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "Let’s dive in by studying the [“Hello World”](https://en.wikipedia.org/wiki/%22Hello,_World!%22_program) of regression\n",
    "algorithms: linear regression\n",
    "\n",
    "Suppose we would like to predict the log of the sale price of a home, given\n",
    "only the livable square footage of the home\n",
    "\n",
    "The linear regression model for this situation is\n",
    "\n",
    "$$\n",
    "\\log(\\text{price}) = \\beta_0 + \\beta_1 \\text{sqft_living} + \\epsilon\n",
    "$$\n",
    "\n",
    "$ \\beta_0 $ and $ \\beta_1 $ are called parameters (also coefficients or\n",
    "weights) and it is the task of the machine learning algorithm to find the values\n",
    "for the parameters that best fit the data\n",
    "\n",
    "$ \\epsilon $ is the error term. It would be unusual for the observed\n",
    "$ \\log(\\text{price}) $ will be an exact linear function of\n",
    "$ \\text{sqft_living} $. The error term captures the deviation of\n",
    "$ \\log(\\text{price}) $ from a linear function of $ \\text{sqft_living} $.\n",
    "\n",
    "The linear regression algorithm will choose the parameters to minimize the\n",
    "*mean squared error* (MSE) function, which for our example is written\n",
    "\n",
    "$$\n",
    "\\frac{1}{N} \\sum_{i=1}^N \\left(\\log(\\text{price}_i) - (\\beta_0 + \\beta_1 \\text{sqft_living}_i) \\right)^2\n",
    "$$\n",
    "\n",
    "The output of this algorithm is the straight line (hence linear) that passes as\n",
    "close to the points on our scatter chart as possible\n",
    "\n",
    "The `sns.lmplot` function below will plot our scatter chart and draw the\n",
    "optimal linear regression line through the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "sns.lmplot(\n",
    "    data=df, x=\"sqft_living\", y=\"log_price\", height=6,\n",
    "    scatter_kws=dict(s=1.5, alpha=0.35)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s use `sklearn` to replicate the figure ourselves\n",
    "\n",
    "First we fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# import\n",
    "from sklearn import linear_model\n",
    "\n",
    "# construct the model instance\n",
    "sqft_lr_model = linear_model.LinearRegression()\n",
    "\n",
    "# fit the model\n",
    "sqft_lr_model.fit(X[[\"sqft_living\"]], y)\n",
    "\n",
    "# print the coefficients\n",
    "beta_0 = sqft_lr_model.intercept_\n",
    "beta_1 = sqft_lr_model.coef_\n",
    "\n",
    "print(f\"Fit model: log(price) = {beta_0} + {beta_1} sqft_living\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = var_scatter(df)\n",
    "\n",
    "# points for the line\n",
    "x = np.array([0, df[\"sqft_living\"].max()])\n",
    "ax.plot(x, beta_0 + beta_1*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call the `predict` method on our model to evaluate the model at\n",
    "arbitrary points\n",
    "\n",
    "For example, we can ask the model to predict the sale price of a 5,000 square\n",
    "foot home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# note, the argument needs to be two-dimensional, you'll see why shortly\n",
    "logp_5000 = sqft_lr_model.predict([[5000]])[0]\n",
    "print(f\"The model predicts a 5,000 sq. foot home would cost {np.exp(logp_5000)} dollars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>\n",
    "\n",
    "**Check for understanding**\n",
    "\n",
    "Use the `sqft_lr_model` that we fit to generate predictions for all data points\n",
    "in our sample\n",
    "\n",
    "Note that you need to pass the `predict` a DataFrame (not Series)\n",
    "containing the `sqft_living` column – (see how we passed that to `.fit`\n",
    "above for help)\n",
    "\n",
    "Make a scatter chart with the actual data and the predictions on the same\n",
    "figure. Does it look familiar?\n",
    "\n",
    "When making the scatter for model predictions we recommend passing\n",
    "`c=\"red\"` and `alpha=0.25` so you can distinguish the data from\n",
    "predictions\n",
    "\n",
    "\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# generate predictions\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# make scatter of data\n",
    "\n",
    "# make scatter of predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>\n",
    "\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>\n",
    "\n",
    "**Check for understanding**\n",
    "\n",
    "Use the `metrics.mean_squared_error` function to evaluate the loss\n",
    "function used by `sklearn` when it fit the model for us\n",
    "\n",
    "Read the docstring to learn what the arguments to that function should be\n",
    "\n",
    "\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate linear regression\n",
    "\n",
    "The example we have been working with is referred to as univariate linear\n",
    "regression because we used a single feature\n",
    "\n",
    "In practice more features would be used\n",
    "\n",
    "Suppose that in addition to `sqft_living` we also wanted to use the `bathrooms` variable\n",
    "\n",
    "In this case the linear regression model is\n",
    "\n",
    "$$\n",
    "\\log(\\text{price}) = \\beta_0 + \\beta_1 \\text{sqft_living} +\n",
    "\\beta_2 \\text{bathrooms} + \\epsilon\n",
    "$$\n",
    "\n",
    "We could keep adding one variable at a time and adding a new $ \\beta_{j} $ coefficient for the $ j $ th variable, but there’s an easier way\n",
    "\n",
    "Let’s write this equation in vector/matrix form as\n",
    "\n",
    "$$\n",
    "\\underbrace{\\begin{bmatrix} \\log(\\text{price}_1) \\\\ \\log(\\text{price}_2) \\\\ \\vdots \\\\ \\log(\\text{price}_N)\\end{bmatrix}}_Y = \\underbrace{\\begin{bmatrix} 1 & \\text{sqft_living}_1 & \\text{bathrooms}_1 \\\\ 1 & \\text{sqft_living}_2 & \\text{bathrooms}_2 \\\\ \\vdots & \\vdots & \\vdots \\\\ 1 & \\text{sqft_living}_N & \\text{bathrooms}_N \\end{bmatrix}}_{X} \\underbrace{\\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\end{bmatrix}}_{\\beta} + \\epsilon\n",
    "$$\n",
    "\n",
    "Notice that we can add as many columns to $ X $ as we’d like and the linear\n",
    "regression model will still be written $ Y = X \\beta + \\epsilon $\n",
    "\n",
    "The mean squared error loss function for the general model is\n",
    "\n",
    "$$\n",
    "\\frac{1}{N} \\sum_{i=1}^N (y_i - X_i \\beta)^2 = \\frac{1}{N} {|| y - X \\beta||_2}^2\n",
    "$$\n",
    "\n",
    "where $ || \\cdot ||_2 $ is the [l2-norm](http://mathworld.wolfram.com/L2-Norm.html)\n",
    "\n",
    "Let’s fit the linear regression model using all the columns in `X`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "lr_model = linear_model.LinearRegression()\n",
    "lr_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just fit a model with 18 variables and it was just as fast and easy as\n",
    "fitting the model with 1 variable!\n",
    "\n",
    "It is difficult to visualize a 18-dimensional model, but just so we can see the\n",
    "difference the extra features made let’s make the log price vs `sqft_living`\n",
    "one more time, including the prediction from both of our linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "ax = var_scatter(df)\n",
    "\n",
    "def scatter_model(mod, X, ax=None, color=\"green\", x=\"sqft_living\"):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "\n",
    "    ax.scatter(X[x], mod.predict(X), c=color, alpha=0.25, s=1)\n",
    "    return ax\n",
    "\n",
    "scatter_model(lr_model, X, ax, color=\"green\")\n",
    "scatter_model(sqft_lr_model, X[[\"sqft_living\"]], ax, color=\"red\")\n",
    "ax.legend([\"data\", \"full model\", \"sqft model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>\n",
    "\n",
    "**Check for understanding**\n",
    "\n",
    "Compare the mean squared error for the `lr_model` and the `sqft_lr_model`\n",
    "\n",
    "Which model has a better fit? Defend your answer to your neighbor\n",
    "\n",
    "\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear relationships in linear regression\n",
    "\n",
    "While it sounds like an oxymoron, it is possible to include non-linear features\n",
    "in a linear regression model\n",
    "\n",
    "The distinguishing feature of the linear regression model is that the each\n",
    "prediction is generated by taking the dot product (a linear operator) between a\n",
    "feature vector (one row of $ X $) and a coefficient vector ($ \\beta $)\n",
    "\n",
    "There is, however, no restriction on what element we include in our feature\n",
    "vector\n",
    "\n",
    "Let’s consider an example…\n",
    "\n",
    "Starting from the `sqft_living` only model, suppose we have a hunch that we\n",
    "should also include the *percent of square feed above ground*\n",
    "\n",
    "This last variable can be computed as `sqft_above / sqft_living`\n",
    "\n",
    "This second feature is nonlinear, but could easily be included as a column in\n",
    "`X`\n",
    "\n",
    "Let’s see this in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "X2 = X[[\"sqft_living\"]].copy()\n",
    "X2[\"pct_sqft_above\"] = X[\"sqft_above\"] / X[\"sqft_living\"]\n",
    "\n",
    "sqft_above_lr_model = linear_model.LinearRegression()\n",
    "sqft_above_lr_model.fit(X2, y)\n",
    "\n",
    "new_mse = metrics.mean_squared_error(y, sqft_above_lr_model.predict(X2))\n",
    "old_mse = metrics.mean_squared_error(y, sqft_lr_model.predict(X2[[\"sqft_living\"]]))\n",
    "print(f\"The mse changed from {old_mse} to {new_mse} by including our new feature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>\n",
    "\n",
    "**Check for understanding**\n",
    "\n",
    "Explore how the fit of the full model can be improved by adding additional\n",
    "features created from the existing ones\n",
    "\n",
    "\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>\n",
    "\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of determining what columns belong in $ X $ is called *feature\n",
    "engineering* and is a large part of a machine learning practitioner’s job\n",
    "\n",
    "You may recall from (or will see in) your econometrics course(s) that\n",
    "the choice of which control variables to include in a regression model\n",
    "is an important part of applied research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretability\n",
    "\n",
    "Before moving to our next regression model, we want to touch on the idea of\n",
    "the **interpretability** of models\n",
    "\n",
    "A model that is interpretable is a model for which we can analyze the\n",
    "coefficients and explain why it makes its predictions\n",
    "\n",
    "Recall $ \\beta_0 $ and $ \\beta_1 $ from the univariate model\n",
    "\n",
    "The interpretation of the model is that $ \\beta_0 $ captures the notion of\n",
    "the average or starting house price and $ \\beta_1 $ is the additional value\n",
    "per square foot\n",
    "\n",
    "Concretely we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "beta_0, beta_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which means that our model predicts the log price of a house to be 12.22, plus\n",
    "an additional 0.0004 for every square foot\n",
    "\n",
    "Some more exotic machine learning methods are potentially more accurate, but\n",
    "less interpretable\n",
    "\n",
    "The accuracy vs interpretably tradeoff is a hot discussion topic, especially as\n",
    "it relates to things like ethics in machine learning and is something you\n",
    "should be aware of as continue to learn about these techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression\n",
    "\n",
    "Lasso regression is very closely related to linear regression\n",
    "\n",
    "The lasso model also generates predictions using $ y = X \\beta $, but it\n",
    "optimizes over a slightly different loss function\n",
    "\n",
    "The optimization problem solved by lasso regression can be written\n",
    "\n",
    "$$\n",
    "\\min_{\\beta} {|| X \\beta - y||_2}^2 + \\underbrace{\\alpha {|| \\beta ||_1}}_{\\text{new part}}\n",
    "$$\n",
    "\n",
    "where $ || a ||_1 = \\sum_{i=1}^N | a_i| $ is the [l1-norm](http://mathworld.wolfram.com/L1-Norm.html) and $ \\alpha $ is called the regularization parameter\n",
    "\n",
    "The additional term penalizes large coefficients and in practice has the effect\n",
    "of setting coefficients to zero for features that are not informative about the\n",
    "target\n",
    "\n",
    "Let’s see an example of what this looks like using the full feature set in\n",
    "`X`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "lasso_model = linear_model.Lasso()\n",
    "lasso_model.fit(X, y)\n",
    "\n",
    "lasso_coefs = pd.Series(dict(zip(list(X), lasso_model.coef_)))\n",
    "lr_coefs = pd.Series(dict(zip(list(X), lr_model.coef_)))\n",
    "coefs = pd.DataFrame(dict(lasso=lasso_coefs, linreg=lr_coefs))\n",
    "coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that many of the coefficients from the lasso regression have been set to\n",
    "zero\n",
    "\n",
    "The intuition here is that the corresponding features must not have provided\n",
    "enough predictive power to be worth considering alongside the other features\n",
    "\n",
    "The default value for the $ \\alpha $ parameter is 1.0\n",
    "\n",
    "Larger values of the parameter will cause coefficients to shrink (and maybe\n",
    "additional ones to be thrown out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Compute lasso for many alphas (the lasso path)\n",
    "from itertools import cycle\n",
    "alphas = np.exp(np.linspace(10, -2, 50))\n",
    "alphas, coefs_lasso, _ = linear_model.lasso_path(X, y, alphas=alphas, fit_intercept=True, max_iter=10000)\n",
    "\n",
    "# plotting\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "colors = cycle(sns.color_palette(\"colorblind\", 16))\n",
    "log_alphas = -np.log10(alphas)\n",
    "for coef_l, c, name in zip(coefs_lasso, colors, list(X)):\n",
    "       plt.plot(log_alphas, coef_l, c=c)\n",
    "       plt.xlabel('-Log(alpha)')\n",
    "       plt.ylabel('lasso coefficients')\n",
    "       plt.title('Lasso Path')\n",
    "       plt.axis('tight')\n",
    "       maxabs = np.max(np.abs(coef_l))\n",
    "       i = [idx for idx in range(len(coef_l)) if abs(coef_l[idx]) >= (0.9*maxabs)][0]\n",
    "       xnote = log_alphas[i]\n",
    "       ynote = coef_l[i]\n",
    "       plt.annotate(name, (xnote, ynote), color=c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting and regularization\n",
    "\n",
    "You might be asking yourself “Why would we ever want to throw variables out,\n",
    "can’t that only hurt our model?”\n",
    "\n",
    "The primary answer is to help us avoid a common issue called **overfitting**\n",
    "\n",
    "Overfitting refers to a model that specializes its coefficients too much on the\n",
    "data it was trained on, and then performs poorly when predicting on data\n",
    "outside the training set\n",
    "\n",
    "The extreme example of overfitting is a model that can perfectly memorize the\n",
    "training data, but can do no better than just randomly guess when predicting\n",
    "on a new observation\n",
    "\n",
    "The techniques applied to reduce overfitting are known as **regularization**\n",
    "\n",
    "Regularization is an attempt to limit a model’s ability to specialize too narrowly\n",
    "on training data (e.g. limit overfitting) by penalizing extreme values of the\n",
    "model’s parameters\n",
    "\n",
    "The additional term in the lasso regression loss function ($ \\alpha ||\\beta||_1 $)\n",
    "is a form of regularization\n",
    "\n",
    "Let’s demonstrate the overfitting and regularization phenomenon on our housing\n",
    "price data as follows:\n",
    "\n",
    "1. Split the data set into training and testing subsets. We will use the first 50 observations for training, and the rest for testing  \n",
    "1. Fit the linear regression model and report MSE on training and testing datasets  \n",
    "1. Fit the lasso model and report the same statistics  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def fit_and_report_mses(mod, X_train, X_test, y_train, y_test):\n",
    "    mod.fit(X_train, y_train)\n",
    "    return dict(\n",
    "        mse_train=metrics.mean_squared_error(y_train, mod.predict(X_train)),\n",
    "        mse_test=metrics.mean_squared_error(y_test, mod.predict(X_test))\n",
    "    )\n",
    "\n",
    "n_test = 50\n",
    "X_train = X.iloc[:n_test, :]\n",
    "X_test = X.iloc[n_test:, :]\n",
    "y_train = y.iloc[:n_test]\n",
    "y_test = y.iloc[n_test:]\n",
    "\n",
    "fit_and_report_mses(linear_model.LinearRegression(), X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "fit_and_report_mses(linear_model.Lasso(), X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the MSE on the training dataset was smaller for the linear model\n",
    "without the regularization, but that the MSE on the test dataset was much\n",
    "higher\n",
    "\n",
    "This is a strong indication that the linear regression model was\n",
    "overfitting\n",
    "\n",
    "The regularization parameter has a large impact on overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "alphas = np.exp(np.linspace(10, -5, 100))\n",
    "mse = pd.DataFrame([fit_and_report_mses(linear_model.Lasso(alpha=alpha, max_iter=50000),\n",
    "                           X_train, X_test, y_train, y_test)\n",
    "                    for alpha in alphas])\n",
    "mse[\"log_alpha\"] = -np.log10(alphas)\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "colors = sns.color_palette(\"colorblind\", 16)\n",
    "mse.plot(x=\"log_alpha\", y=\"mse_test\", c=colors[0], ax=ax)\n",
    "mse.plot(x=\"log_alpha\", y=\"mse_train\", c=colors[1], ax=ax)\n",
    "ax.set_xlabel(r\"$-\\log(\\alpha)$\")\n",
    "ax.set_ylabel(\"MSE\")\n",
    "ax.get_legend().remove()\n",
    "ax.annotate(\"test\",(mse.log_alpha[15], mse.mse_test[15]),color=colors[0])\n",
    "ax.annotate(\"train\",(mse.log_alpha[30], mse.mse_train[30]),color=colors[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation of regularization parameter\n",
    "\n",
    "As you can see in the above figure, the regularization parameter has a\n",
    "large impact on MSE in the test data. Moreoever, the relationship\n",
    "between the test data MSE and $ \\alpha $ is complicated and\n",
    "non-monotonic. A popular method for choosing the regularization\n",
    "parameter is cross-validation. Roughly speaking, cross-validation\n",
    "splits the dataset into many training/testing subsets, and then choose\n",
    "the value of regularization parameter that minimizes the average\n",
    "MSE. More precisely k-fold cross-validation does the following:\n",
    "\n",
    "1. Partition the dataset randomly into k subsets/”folds”  \n",
    "1. Compute $ MSE_j(\\alpha)= $ mean squared error in j-th subset\n",
    "  when using the j-th subset as test data, and other k-1 as training\n",
    "  data  \n",
    "1. Minimize average (across folds) MSE $ \\min_\\alpha \\frac{1}{k}\n",
    "  \\sum_{j=1}^k MSE_j(\\alpha) $  \n",
    "\n",
    "\n",
    "The following code plots 5-fold cross-validated MSE as a function of\n",
    "$ \\alpha $ using the same training data as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "mse[\"cv\"] = [-np.mean(cross_val_score(linear_model.Lasso(alpha=alpha, max_iter=50000),\n",
    "                                  X_train, y_train, cv=5, scoring='neg_mean_squared_error'))\n",
    "          for alpha in alphas]\n",
    "mse.plot(x=\"log_alpha\", y=\"cv\", c=colors[2], ax=ax)\n",
    "ax.annotate(\"cross-validation\", (mse.log_alpha[40], mse.cv[40]), color=colors[2])\n",
    "ax.get_legend().remove()\n",
    "ax.set_xlabel(r\"$-\\log(\\alpha)$\")\n",
    "ax.set_ylabel(\"MSE\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit learn also includes methods to automate the above and select\n",
    "$ \\alpha $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# LassoCV exploits special structure of lasso problem to minimize CV more efficiently\n",
    "lasso = linear_model.LassoCV(cv=5).fit(X_train,y_train)\n",
    "-np.log10(lasso.alpha_) # should roughly = minimizer on graph, not exactly equal due to random splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holdout\n",
    "\n",
    "Another common technique that practitioners use to avoid overfitting is called\n",
    "*holdout*\n",
    "\n",
    "We demonstrated an extreme example of applying holdout above when we used only\n",
    "the first 50 observations to train our models\n",
    "\n",
    "In general, good practice is to split the entire dataset into a training subset\n",
    "and testing or validation subset\n",
    "\n",
    "The splitting should be done randomly and should leave enough data in the\n",
    "training dataset to produce a good model, but also enough in the validation\n",
    "subset to determine the degree of overfitting\n",
    "\n",
    "There aren’t hard and fast rules for how much data to put in each subset, but a\n",
    "reasonable default would be to use about %75 of the data for training and the\n",
    "rest for testing\n",
    "\n",
    "As in the example above, the training data is often further split\n",
    "while selecting regularization parameters with cross-validation\n",
    "\n",
    "The `sklearn` function `model_selection.train_test_split` will do this for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# note test_size=0.25 is the default value, but is shown here so you\n",
    "# can see how to change it\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>\n",
    "\n",
    "**Check for understanding**\n",
    "\n",
    "Experiment with how the size of the holdout dataset can impact a diagnosis\n",
    "of overfitting\n",
    "\n",
    "Evaluate only the `LinearRegression` model on the full feature set and use\n",
    "the `model_selection.train_test_split` function with various values for\n",
    "`test_size`\n",
    "\n",
    "\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "Random forests are also becoming increasingly popular in\n",
    "economics. This is largely due to the work by Susan Athey and her\n",
    "coauthors. [[AI17]](#athey2017) gives a very brief overview of some of\n",
    "this work, and [[AI18]](#athey2018) are video lectures and associated\n",
    "code aimed at a broad audience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Trees\n",
    "\n",
    "To understand a forest, we must first understand trees.\n",
    "\n",
    "We will begin to understand trees by looking at one.\n",
    "\n",
    "We will a fit a tree to this simulated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Simulate some data and plot it\n",
    "n = 1000\n",
    "Xsim = np.random.rand(n,2)\n",
    "def Ey_x(x):\n",
    "      return 1/3*(np.sin(5*x[0])*np.sqrt(x[1])*np.exp(-(x[1]-0.5)**2))\n",
    "\n",
    "ysim = np.apply_along_axis(Ey_x, 1, Xsim) + np.random.randn(n)*0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import plotly as py\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "def surface_scatter_plot(X,y,f, xlo=0., xhi=1., ngrid=50,\n",
    "                         width=1000, height=800, f0=Ey_x, show_f0=False):\n",
    "    scatter = go.Scatter3d(x=X[:,0],y=X[:,1],z=y,\n",
    "                           mode='markers',\n",
    "                           marker=dict(size=2, opacity=0.3)\n",
    "    )\n",
    "    xgrid = np.linspace(xlo,xhi,ngrid)\n",
    "    ey = np.zeros((len(xgrid),len(xgrid)))\n",
    "    ey0 = np.zeros((len(xgrid),len(xgrid)))\n",
    "    for i in range(len(xgrid)):\n",
    "        for j in range(len(xgrid)):\n",
    "            ey[j,i] = f([xgrid[i],xgrid[j]])\n",
    "            ey0[j,i]= f0([xgrid[i],xgrid[j]])\n",
    "\n",
    "    surface = go.Surface(x=xgrid, y=xgrid, z=ey, colorscale=\"YlOrRd\", opacity=1.0)\n",
    "    if (show_f0):\n",
    "        surface0 = go.Surface(x=xgrid, y=xgrid, z=ey0, opacity=0.8, colorscale=\"YlOrRd\")\n",
    "        layers = [scatter, surface, surface0]\n",
    "    else:\n",
    "        layers = [scatter, surface]\n",
    "    fig = go.Figure(data=layers,\n",
    "                    layout = go.Layout(\n",
    "                        autosize=True,\n",
    "                        scene = dict(xaxis = dict(title='X1'),\n",
    "                                     yaxis = dict(title='X2'),\n",
    "                                     zaxis = dict(title='Y')),\n",
    "                        width=width,\n",
    "                        height=height))\n",
    "    return(fig)\n",
    "\n",
    "fig = surface_scatter_plot(Xsim,ysim,Ey_x)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit a regression tree to this data, and plot the predicted\n",
    "regression surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "fitted_tree = tree.DecisionTreeRegressor(max_depth=3).fit(Xsim,ysim)\n",
    "fig=surface_scatter_plot(Xsim,ysim,lambda x:\n",
    "                         fitted_tree.predict([x]), show_f0=True)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, predictions from regression trees are piecewise\n",
    "constant on rectangular regions. The boundaries of these regions are\n",
    "determined by a decision tree. The following code displays the\n",
    "decision graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import graphviz\n",
    "    tree_graph = tree.export_graphviz(fitted_tree, out_file=None,\n",
    "                                      feature_names=[\"X1\", \"X2\"],\n",
    "                                      filled=True, rounded=True,\n",
    "                                      special_characters=True)\n",
    "    display(graphviz.Source(tree_graph))\n",
    "except:\n",
    "    print(\"graphviz not installed, cannot display tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression trees are formed iteratively.\n",
    "\n",
    "We begin with a rectangular region $ R $ containing all values of\n",
    "the X. We then choose a feature to split on and where to split. The\n",
    "splitting feature and location are chosen to minimize MSE. We then\n",
    "repeat to generate all the branches.\n",
    "\n",
    "- For each region, solve  \n",
    "\n",
    "\n",
    "$$\n",
    "\\min_{j,s} \\left[ \\min_{c_1} \\sum_{i: x_{i,j} \\leq s, x_i \\in R}\n",
    "    (y_i - c_1)^2 + \\min_{c_2} \\sum_{i: x_{i,j} > s, x_i \\in R}\n",
    "    (y_i - c_2)^2 \\right]\n",
    "$$\n",
    "\n",
    "- Repeat with each of the two smaller rectangles  \n",
    "- Stop when $ |R| = $ some chosen minimum size or when depth of tree $ = $\n",
    "  some chosen maximum  \n",
    "- Prune tree  \n",
    "\n",
    "\n",
    "$$\n",
    "\\min_{tree \\subset T} \\sum (\\hat{f}(x)-y)^2 + \\alpha|\\text{terminal\n",
    "   nodes in tree}|\n",
    "$$\n",
    "\n",
    "There are many variations on this tree building algorithm. They all\n",
    "share some rule to decide on which variable and where to split. They\n",
    "all have some kind of stopping rule, but not necessarily the same\n",
    "one. For example, some algorithms stop splitting into new branches\n",
    "when the improvement in MSE becomes small.\n",
    "\n",
    "As with lasso, regression trees involve some regularization. In the\n",
    "above description, the minimum leaf size, maximum tree depth, and\n",
    "$ \\alpha $ in the pruning step serve as regularization\n",
    "parameters.\n",
    "\n",
    "<blockquote>\n",
    "\n",
    "**Check for understanding**\n",
    "\n",
    "Read the documentation for sklearn.tree.DecisionTreeRegressor, and\n",
    "then experiment to see how adjusting some of the regularization parameters\n",
    "affect the fitted tree.\n",
    "\n",
    "\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# plot trees when varying some regularization parameter(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>\n",
    "\n",
    "**Check for understanding**\n",
    "\n",
    "Fit a regression tree to the housing price data and use graphviz\n",
    "to visualize the decision graph.\n",
    "\n",
    "\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An advantage of regression trees (and random forests) is that they\n",
    "adapt automatically to feature scales and units. Last class, a student\n",
    "pointed out that it was strange to include the numeric zipcode as a\n",
    "variable in the linear regression and lasso. It would have made more\n",
    "sense to include indicator or dummy variables for each\n",
    "zipcode. Regression trees do not impose linearity or even\n",
    "monotonicity, so it less harmful to have the numeric zipcode as a\n",
    "feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "ax = var_scatter(df, var=\"zipcode\")\n",
    "zip_tree = tree.DecisionTreeRegressor(max_depth=10).fit(X[[\"zipcode\"]],y)\n",
    "scatter_model(zip_tree, X[[\"zipcode\"]], ax, x=\"zipcode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "\n",
    "A random forests is the average of many randomized regression trees\n",
    "\n",
    "Trees randomized by\n",
    "- Fitting on randomly resampled subsets of data\n",
    "- Randomize features chosen for branching:\n",
    "\n",
    "$$\n",
    "\\min_{j \\in S,s} \\left[ \\min_{c_1} \\sum_{i: x_{i,j} \\leq s, x_i \\in R}\n",
    "    (y_i - c_1)^2 + \\min_{c_2} \\sum_{i: x_{i,j} > s, x_i \\in R}\n",
    "    (y_i - c_2)^2 \\right]\n",
    "$$\n",
    "\n",
    "where $ S $ is a random subset of features\n",
    "\n",
    "Randomizing and averaging smooths out the predictions from individual\n",
    "trees. This improves predictions and reduces the variance of the\n",
    "predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# example of forest for simulated data\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest = RandomForestRegressor(n_estimators = 10).fit(Xsim,ysim)\n",
    "fig=surface_scatter_plot(Xsim,ysim,lambda x: forest.predict([x]),\n",
    "                         show_f0=True)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests generally produce more accurate predictions than any\n",
    "single tree. However, random forests have at least two downsides\n",
    "compared to trees. Random forests take longer to compute, and random\n",
    "forests can be more difficult to interpret. We can no longer draw a\n",
    "single decision graph. Instead, people often report “feature\n",
    "importance” for random forests. Feature importance is the average\n",
    "across trees of how much the splits on each feature decreased\n",
    "MSE. Greater importance of a given feature means that the trees split\n",
    "on that feature more often and/or splitting on that feature resulted\n",
    "in larger decreases in MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "forest.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>\n",
    "\n",
    "**Check for understanding**\n",
    "\n",
    "Fit a random forest to the housing price data. Compare the MSE on\n",
    "a testing set to that of Lasso.\n",
    "\n",
    "\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# Fit random forest and compute MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>\n",
    "Produce a bar chart of feature importances for predicting house\n",
    "prices.\n",
    "\n",
    "\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>\n",
    "\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "The final regression algorithm we will talk about in this lecture is a type of\n",
    "neural network.\n",
    "\n",
    "Based on your interest in this course, our strong prior is that you have\n",
    "probably heard about neural networks in the news or social media.\n",
    "\n",
    "The purpose of this section is not to give an exhaustive overview of the topic,\n",
    "but instead to introduce you to a particular neural network model and present\n",
    "it from a different perspective that hopefully complement materials you may run\n",
    "into elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Background\n",
    "\n",
    "If linear regression is the [“Hello World”](https://en.wikipedia.org/wiki/%22Hello,_World!%22_program) of regression\n",
    "algorithms, then the multi-layer perceptron (MLP) is the hello world of neural\n",
    "networks.\n",
    "\n",
    "We’ll start with a single (hidden) layer MLP and then build up to the general form.\n",
    "\n",
    "The prediction function for a single layer MLP is\n",
    "\n",
    "$$\n",
    "y = f_1(X w_1 + b_1) w_2 + b_2\n",
    "$$\n",
    "\n",
    "In words what we have here is *nested linear regression* (the $ (\\cdot) w_i + b_i $\n",
    "parts), separated by an *activation function* (the $ f_1 $).\n",
    "\n",
    "Let’s unpack what happens, starting from our $ N_\\text{samples} \\times\n",
    "N_\\text{features} $ feature matrix $ X $.\n",
    "\n",
    "1. First, $ X $ is multiplied by a coefficient matrix $ w_1 $. $ w_1 $ is often called the *weight matrix* or *weights* for short and has dimension $ N_{\\text{features}} \\times N_1 $  \n",
    "1. The vector $ b_1 $ is added to each row. $ b_1 $ is often called the *bias vector* or *bias* for short  and has dimension $ N_1 \\times 1 $  \n",
    "1. The function $ f_1 $ is then applied. Typically $ f_1 $ a non-linear function that is applied separately to each element. $ f_1 $ is called the *activation function*  \n",
    "1. The output is then multiplied by a weight matrix $ w_2 $ with dimension $ N_1 \\times 1 $  \n",
    "1. Finally a scalar $ b_2 $ is added to each row to generate the final prediction with dimension $ N_{\\text{samples}} \\times 1 $  \n",
    "\n",
    "\n",
    "The way we might write this in python is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide-output": false
   },
   "source": [
    "```python\n",
    "y = f(X@w1 + b1)@w2 + b2\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build an \\$N\\$-hidden layer MLP we will *nest* additional linear regressions separated by activation functions.\n",
    "\n",
    "The equation for this case is difficult to express, but has the following form\n",
    "\n",
    "$$\n",
    "y = f_{\\cdots} \\left(f_2(f_1(X w_1 + b_1) w_2 + b_2) w_{\\cdots} + b_{\\cdots} \\right) w_{N+1} + b_{N+1}\n",
    "$$\n",
    "\n",
    "where the $ \\cdots $ represents layers 3 to $ N $.\n",
    "\n",
    "Notice the pattern of a linear regression ($ (\\cdot) w + b $),\n",
    "followed by applying an activation function ($ f $) at each step.\n",
    "\n",
    "<blockquote>\n",
    "\n",
    "**Check for understanding**\n",
    "\n",
    "Fill in the blanks in the pseudo code below for the generic MLP\n",
    "\n",
    "Note that this is inside a markdown cell because the code is not valid\n",
    "python\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "ws = [w1, w2, ..., wend]\n",
    "bs = [b1, b2, ..., bend]\n",
    "\n",
    "def eval_mlp(X, ws, bs, f):\n",
    "    \"\"\"\n",
    "    evaluate MLP given weights (ws), bias (bs) and an activation (f)\n",
    "\n",
    "    Assumes that the same activation is applied to all hidden layers\n",
    "    \"\"\"\n",
    "    N = len(ws) - 1\n",
    "\n",
    "    out = X\n",
    "    for i in range(N):\n",
    "        out = f(__)  # replace the __\n",
    "\n",
    "    # For this step remember python starts counting at 0!\n",
    "    return out@__ + __  # replace the __\n",
    "```\n",
    "\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss or error function typically used when using an MLP for regression is\n",
    "our now familiar mean squared error loss function:\n",
    "\n",
    "$$\n",
    "{||y - \\hat{y}||_2}^2\n",
    "$$\n",
    "\n",
    "where $ \\hat{y} $ is the output of the neural network.\n",
    "\n",
    "Here we fit a neural network to the same simulated data that we used\n",
    "in the random forests section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from sklearn import neural_network\n",
    "nn = neural_network.MLPRegressor((6,), activation=\"logistic\",\n",
    "                                 verbose=True, solver=\"lbfgs\",\n",
    "                                 alpha=0.0).fit(Xsim,ysim)\n",
    "fig=surface_scatter_plot(Xsim,ysim,lambda x: nn.predict([x]), show_f0=True)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are nearly ready to test out a MLP on our housing data, but there are a few\n",
    "more talking points to cover:\n",
    "\n",
    "- [[HSW89]](#hornik1989) show that MLPs are universal approximators,\n",
    "  meaning they are theoretically capable of approximating any\n",
    "  function. This fact is sometimes stated as though it helps explain\n",
    "  the exceptionally good predictive ability of neural networks. Do not\n",
    "  be fooled by this fallacy. Many other methods are universal approximators,\n",
    "  including regression trees and more classic statistical methods like\n",
    "  serires regression and kernel regression. The explanation for neural\n",
    "  networks’ predictive success lies elsewhere.  <sup>[1](#rate)</sup>  \n",
    "- It is crucial that the activation functions are non-linear. If they were not\n",
    "  the MLP would be combining linear combinations of linear combinations and\n",
    "  would always be linear.  \n",
    "- The hidden layer structure of a MLP allows it to do automatic feature engineering.\n",
    "  This is in contrast to the example we had above where we manually engineered\n",
    "  the square feet above ground feature.  \n",
    "\n",
    "\n",
    "<a id='rate'></a>\n",
    "**[1]** Two facts about neural networks that are relevant to their\n",
    "predictive success: automatic feature engineering, as\n",
    "mentioned above, and the ability of neural networks to\n",
    "approximate a broad, but not quite universal, class of\n",
    "functions with relatively few parameters. This allows\n",
    "neural networks to have a fast statistical convergence\n",
    "rate. Under appropriate assumptions, lasso, series\n",
    "regression, and kernel regression share this fast\n",
    "convergence rate property, but they lack automatic feature\n",
    "engineering. Random forests have automatic feature\n",
    "engineering, but do not have a fast convergence rate.\n",
    "Neural networks are somewhat unique in combining both\n",
    "properties.\n",
    "See\n",
    "[these notes and references therein](http://faculty.arts.ubc.ca/pschrimpf/628/machineLearningAndCausalInference.html#2_introduction_to_machine_learning)\n",
    "for more information about convergence rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application\n",
    "\n",
    "Ok, now let’s try out our first neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from sklearn import neural_network\n",
    "\n",
    "X = df.drop([\"price\", \"date\", \"id\", \"log_price\"], axis=1).copy()\n",
    "for col in list(X):\n",
    "    X[col] = X[col].astype(float)\n",
    "y = np.log(df[\"price\"])\n",
    "\n",
    "# two hidden layers, with N1=30 and N2=20\n",
    "nn_model = neural_network.MLPRegressor((30, 20))\n",
    "nn_model.fit(X, y)\n",
    "\n",
    "ax = var_scatter(df)\n",
    "scatter_model(nn_model, X, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! That plot looks horrible, let’s check the MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "mse_nn = metrics.mean_squared_error(y, nn_model.predict(X))\n",
    "mse_nn / metrics.mean_squared_error(y, lr_model.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So… after all that talk about neural networks begin able to do anything, we\n",
    "get a mean squared error that is tens of thousands of times larger than the\n",
    "MSE from a linear regression!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input scaling\n",
    "\n",
    "The issue here is that neural networks are extremely sensitive to the scale\n",
    "(both relative and absolute) of the input features.\n",
    "\n",
    "The reasons for why are a bit beyond the scope of this lecture, but the main\n",
    "idea is the training procedure will pay too much attention to relatively larger\n",
    "features (relative scale) and become unstable if features are very large\n",
    "(absolute scale).\n",
    "\n",
    "A common technique to overcome this issue is to scale each variable so that the\n",
    "observations have mean 0 and standard deviation 1.\n",
    "\n",
    "This is known as scaling or normalizing the inputs.\n",
    "\n",
    "We saw this already in the [sklearn concepts](./04_sklearn_concepts.ipynb) example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing, pipeline\n",
    "\n",
    "# the pipeline defines any number of steps that will be applied\n",
    "# to transform the `X` data and then a final step that is a model\n",
    "# we can use for prediction\n",
    "nn_scaled_model = pipeline.make_pipeline(\n",
    "    preprocessing.StandardScaler(),  # this will do the input scaling\n",
    "    neural_network.MLPRegressor((30, 20))  # put your favorite model here\n",
    ")\n",
    "\n",
    "# we can now use `model` like we have used our other models all along\n",
    "# call fit\n",
    "nn_scaled_model.fit(X, y)\n",
    "\n",
    "# call predict\n",
    "mse_nn_scaled = metrics.mean_squared_error(y, nn_scaled_model.predict(X))\n",
    "\n",
    "print(f\"Unscaled mse {mse_nn}\")\n",
    "print(f\"Scaled mse {mse_nn_scaled}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we have it, much better. This is the smallest MSE we have seen so far.\n",
    "\n",
    "A scatter plot of the predictions looks very similar to the observed prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "ax = var_scatter(df)\n",
    "scatter_model(nn_scaled_model, X, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tradeoffs\n",
    "\n",
    "So we’ve seen that neural networks are very flexible and can approximate highly\n",
    "nonlinear functions\n",
    "\n",
    "However, there are tradeoffs to using them\n",
    "\n",
    "We’ll discuss a few of them here\n",
    "\n",
    "- **Interpretability**: unlike linear regression or lasso, neural\n",
    "  networks are not easily interpretable. We could look at the $ w $\n",
    "  matrices or $ b $ vectors, but due to the nested composition and\n",
    "  nonlinear activation functions it is very difficult to interpret\n",
    "  just how each coefficient impacts the output. In settings like\n",
    "  making economic policy recommendations or suggestion decisions with\n",
    "  potentially ethical consequences (e.g. approving loans, screening)\n",
    "  the lack of interpretability can be a non-starter.  \n",
    "- **Efficiency/time**: Neural networks require more computational\n",
    "  power to evaluate (generate predictions) and are orders of magnitude\n",
    "  more expensive to train than classical machine learning\n",
    "  methods.  \n",
    "- **Automated feature engineering**: the nested linear regressions\n",
    "  allows neural networks to learn features of the data that are\n",
    "  composed of arbitrary linear combinations of the original feature\n",
    "  set. The non-linear activation functions allow the network to learn\n",
    "  arbitrary non-linear features. Manual feature engineering is based\n",
    "  largely on the researchers intuition and a fair amount of trial and\n",
    "  error. Coming up with the right features that allow for more\n",
    "  explanatory power without overfitting is very difficult. Neural\n",
    "  networks automate that process by having the data itself guide the\n",
    "  training process to select features that satisfy accuracy and\n",
    "  regularization conditions.  \n",
    "- **Overfitting**: because of their great flexibly and explanatory\n",
    "  power it is very easy to overfit when using neural networks. There\n",
    "  are various approaches to regularization when training neural\n",
    "  networks, and these should be studied and evaluated when building\n",
    "  networks that will be used for decision making purposes.  \n",
    "\n",
    "\n",
    "<blockquote>\n",
    "\n",
    "**Check for understanding**\n",
    "\n",
    "As we did with regression trees above, explore the ability of\n",
    "neural networks to automate feature engineering by using numeric\n",
    "zipcode to predict house prices. Experiment with how adjusting the\n",
    "regularization parameters affects the predictions.\n",
    "\n",
    "\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "\n",
    "Read the documentation for sklearn.neural_network.MLPRegressor, and\n",
    "experiment with how adjusting layer depth, width, and other\n",
    "regularization parameters affects prediction using the full housing\n",
    "data.\n",
    "\n",
    "\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Two good text books covering the above regression methods are\n",
    "[[FHT09]](#friedman2008) and [[EH16]](#efron2016) .\n",
    "\n",
    "<a id='athey2018'></a>\n",
    "\\[AI18\\] Susan Athey and Guido Imbens. Machine learning and econometrics. 2018. URL: [https://www.aeaweb.org/conference/cont-ed/2018-webcasts](https://www.aeaweb.org/conference/cont-ed/2018-webcasts).\n",
    "\n",
    "<a id='athey2017'></a>\n",
    "\\[AI17\\] Susan Athey and Guido W. Imbens. The state of applied econometrics: causality and policy evaluation. *Journal of Economic Perspectives*, 31(2):3–32, May 2017. URL: [http://www.aeaweb.org/articles?id=10.1257/jep.31.2.3](http://www.aeaweb.org/articles?id=10.1257/jep.31.2.3), [doi:10.1257/jep.31.2.3](https://doi.org/10.1257/jep.31.2.3).\n",
    "\n",
    "<a id='belloni2011'></a>\n",
    "\\[BC11\\] Alexandre Belloni and Victor Chernozhukov. *High Dimensional Sparse Econometric Models: An Introduction*, pages 121–156. Springer Berlin Heidelberg, Berlin, Heidelberg, 2011. URL: [https://doi.org/10.1007/978-3-642-19989-9_3](https://doi.org/10.1007/978-3-642-19989-9_3), [doi:10.1007/978-3-642-19989-9_3](https://doi.org/10.1007/978-3-642-19989-9_3).\n",
    "\n",
    "<a id='chernozhukov2018'></a>\n",
    "\\[CCD+18\\] Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. Double/debiased machine learning for treatment and structural parameters. *The Econometrics Journal*, 21(1):C1–C68, 2018. URL: [https://onlinelibrary.wiley.com/doi/abs/10.1111/ectj.12097](https://onlinelibrary.wiley.com/doi/abs/10.1111/ectj.12097), [arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1111/ectj.12097](https://arxiv.org/abs/https://onlinelibrary.wiley.com/doi/pdf/10.1111/ectj.12097), [doi:10.1111/ectj.12097](https://doi.org/10.1111/ectj.12097).\n",
    "\n",
    "<a id='hdm'></a>\n",
    "\\[CHS16\\] Victor Chernozhukov, Chris Hansen, and Martin Spindler. hdm: high-dimensional metrics. *R Journal*, 8(2):185–199, 2016. URL: [https://journal.r-project.org/archive/2016/RJ-2016-040/index.html](https://journal.r-project.org/archive/2016/RJ-2016-040/index.html).\n",
    "\n",
    "<a id='efron2016'></a>\n",
    "\\[EH16\\] Bradley Efron and Trevor Hastie. *Computer age statistical inference*. Volume 5. Cambridge University Press, 2016. URL: [https://web.stanford.edu/~hastie/CASI/](https://web.stanford.edu/~hastie/CASI/).\n",
    "\n",
    "<a id='friedman2008'></a>\n",
    "\\[FHT09\\] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. *The elements of statistical learning*. Springer series in statistics, 2009. URL: [https://web.stanford.edu/~hastie/ElemStatLearn/](https://web.stanford.edu/~hastie/ElemStatLearn/).\n",
    "\n",
    "<a id='hornik1989'></a>\n",
    "\\[HSW89\\] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. *Neural Networks*, 2(5):359 – 366, 1989. URL: [http://www.sciencedirect.com/science/article/pii/0893608089900208](http://www.sciencedirect.com/science/article/pii/0893608089900208), [doi:https://doi.org/10.1016/0893-6080(89)90020-8](https://doi.org/https://doi.org/10.1016/0893-6080%2889%2990020-8)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
