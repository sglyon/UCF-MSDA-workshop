{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Polynomial Regression Example\n",
    "\n",
    "**Spencer Lyon**\n",
    "\n",
    "*UCF MSDA Big Data Seminar*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- In this notebook we will look at an example of how feature engineering is necessary for ML success\n",
    "- We'll start by importing some libraries and importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()\n",
    "\n",
    "from matplotlib.axes._axes import _log as matplotlib_axes_logger\n",
    "matplotlib_axes_logger.setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cubic_data.csv\")\n",
    "df.plot.scatter(x=\"x\", y=\"y\", color=\"k\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We will now use the scikit-learn library to fit a linear model\n",
    "- The model has the form $$y = w_0 + w_1 x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = linear_model.LinearRegression()\n",
    "mod.fit(df[[\"x\"]], df[\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "mod.intercept_, mod.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For this dataset, $w_0 = 0.2896$ and $w_1 = 0.9935$\n",
    "- A common metric for regression tasks is the mean squared error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_prediction = mod.predict(df[[\"x\"]])\n",
    "metrics.mean_squared_error(df[\"y\"], linear_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Below we will visualize the model and its fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "df.plot.scatter(x=\"x\", y=\"y\", ax=ax, color=\"k\")\n",
    "ax.plot(df[\"x\"], linear_prediction);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After examining the dataset, we see that there may be a cubic pattern (why?)\n",
    "- Let's use sklearn to fit a cubic model of the form $$y = w_0 + w_1 x + w_2 x^2 + w_3 x^3$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "cubic = linear_model.LinearRegression()\n",
    "features = df[\"x\"].to_numpy()[:,None] ** np.array([1, 2, 3])[None, :]\n",
    "cubic.fit(features, df[\"y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Let's take a look a the coefficients and mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cubic.intercept_, cubic.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cubic_prediction = cubic.predict(features)\n",
    "metrics.mean_squared_error(df[\"y\"], cubic_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a much better MSE\n",
    "\n",
    "We can also visually see that the fit is better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "df.plot.scatter(x=\"x\", y=\"y\", ax=ax, color=\"k\")\n",
    "ax.scatter(df[\"x\"], cubic_prediction);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outcome\n",
    "\n",
    "- The cubic model fit this dataset far better than the linear model\n",
    "- This was a contrived example (the data is a cubic polynomial, plus some noise)\n",
    "- We knew the underlying relationship was cubic (domain expertiese) and transformed the data appropriately (feature enginnering)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "- A common benefit of Neural networks is that they will do the feature engineering automatically\n",
    "- Let's try it out on this simple dataset\n",
    "- Don't worry about specifics of what the network is doing, we'll get back to that soon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neural_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## np.random.seed(42)\n",
    "nn = neural_network.MLPRegressor(hidden_layer_sizes=(25, 20), max_iter=10000)\n",
    "\n",
    "# fit with only x\n",
    "nn.fit(df[[\"x\"]], df[\"y\"])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "df.plot.scatter(x=\"x\", y=\"y\", ax=ax, color=\"k\")\n",
    "ax.scatter(df[\"x\"], nn.predict(df[[\"x\"]]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- With linear models we could look at model weights (coefficients) to understand behavior\n",
    "- There isn't an obvious/straightforward way to understand what the model is doing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.coefs_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is known as *interpretability* and is something we will come back to\n",
    "- For now we'll look a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "css",
   "language": "python",
   "name": "css"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
